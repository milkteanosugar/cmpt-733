{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install\n",
    "Before we begin, if you don't already have it you will need to install the following packages. Here is the install command:\n",
    "\n",
    "**transformers**: `conda install -c conda-forge transformers`\n",
    "\n",
    "It's important to note that my code differs from Kexin's because I [migrated](https://huggingface.co/transformers/migration.html) to using [HuggingFace's](https://huggingface.co/transformers/index.html) new `transformer` module instead of the formerly known as `pytorch_pretrained_bert` that the author used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read this article for ClinicalBERT\n",
    "https://arxiv.org/pdf/1904.05342.pdf\n",
    "They develop ClinicalBert by applying BERT (bidirectional encoder representations from transformers) to clinical notes. \n",
    "\n",
    "```\n",
    "@article{clinicalbert,\n",
    "author = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},\n",
    "title = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},\n",
    "year = {2019},\n",
    "journal = {arXiv:1904.05342},\n",
    "}\n",
    "```\n",
    "\n",
    "# How My Work Differs from the Author's\n",
    "1. I am not pre-training the ClinicalBERT because the author already performed pre-training on Clinical words and the model's weights are already available.\n",
    "2. I am only working with early clinical notes. \"Discharge summaries have predictive power for readmission. However, discharge summaries might be written after a patient has left the hospital. Therefore, discharge summaries are not actionable since doctors cannot intervene when a patient has left the hospital. Models that dynamically predict readmission in the early stages of a patient's admission are relevant to clinicians...a maximum of the first 48 or 72 hours of a patient's notes are concatenated. These concatenated notes are used to predict readmission.\"[pg 12](https://arxiv.org/pdf/1904.05342.pdf)\n",
    "\n",
    "\n",
    "<img src=\"./images/fig1.png\" width=\"800\" />\n",
    "\n",
    "In this example, care providers add notes to an electronic health record during a patient’s admission, and the model dynamically updates the patient’s risk of being readmitted within a 30-day window.\n",
    "\n",
    "\n",
    "Boag et al. (2018) study the performance of the bag-of-words model, word2vec, and a Long Short-Term Memory Network (lstm) model combined with word2vec on various tasks such as diagnosis prediction and mortality risk estimation. Word embedding models such as word2vec are trained using the local context of individual words, but as clinical notes are long and their words are interdependent (Zhang et al., 2018), these methods cannot capture long-range dependencies.\n",
    "\n",
    "Clinical notes require capturing interactions between distant words.\n",
    "\n",
    "In this work, they develop a model that can predict readmission dynamically. **Making a prediction using a discharge summary at the end of a stay means that there are fewer opportunities to reduce the chance of readmission. To build a clinically-relevant model, we define a task for predicting readmission at any timepoint since a patient was admitted.**\n",
    "\n",
    "Medicine suffers from alarm fatigue (Sendelbach and Funk, 2013). This\n",
    "means useful classification rules for medicine need to have high precision (positive predictive value).\n",
    "\n",
    "Compared to a popular model of clinical text, word2vec, ClinicalBert more accurately captures clinical word similarity.\n",
    "\n",
    "ClinicalBERT is a modified BERT model: Specifically, the representations are learned\n",
    "using medical notes and further processed for downstream clinical tasks.\n",
    "* The transformer encoder architecture is based on a self-attention mechanism\n",
    "* The pre-training objective function for the model is defined using two unsupervised tasks: masked language modeling and next sentence prediction. \n",
    "* The text embeddings and model parameters are fit using stochastic optimization.\n",
    "\n",
    "<img src=\"./images/fig2.png\" width=\"800\" />\n",
    "\n",
    "ClinicalBert learns deep representations of clinical text using two unsupervised language modeling tasks: masked language modeling and\n",
    "next sentence prediction\n",
    "\n",
    "### Clinical Text Embeddings\n",
    "A clinical note input to ClinicalBert is represented as a collection of tokens. In ClinicalBert, a token in a clinical note is computed as\n",
    "the sum of the token embedding, a learned segment embedding, and a position embedding.\n",
    "\n",
    "### Pre-training ClinicalBERT\n",
    "The quality of learned representations of text depends on the text the model was trained on. BERT is trained on BooksCorpus and Wikipedia. However, these two datasets are distinct from clinical notes (where jargon and abbreviations are common). Also clinical notes have different syntax and grammar than common language in books or encyclopedias. It is hard to understand clinical notes without professional training.\n",
    "\n",
    "ClinicalBERT improves over BERT on the MIMIC-III corpus of clinical notes for \n",
    "1. Accuracy of masked language modeling a.k.a. predicting held-out tokens (86.80% vs 56.80%).\n",
    "2. Next sentence prediction (99.25% vs. 80.50%).\n",
    "The pre-training objective function based on the two tasks is the sum of the log-likelihood of the masked tokens and the log-likelihood of the binary variable indicating whether two sentences are consecutive.\n",
    "\n",
    "### Fine-tuning ClinicalBERT\n",
    "The model parameters are fine-tuned to maximize the log-likelihood of this binary classifier: equation (2)\n",
    "\n",
    "##  Empirical Study II: 30-Day Hospital Readmission Prediction\n",
    "Before the author even evaluated ClinicalBERT's performance as a model of readmission, **his initial experiment showed that the original BERT suffered in performance on the masked language modeling task on the MIMIC-III data as well as the next sentence prediction tasks. This proves the need develop models tailored to clinical data such as ClinicalBERT!**\n",
    "\n",
    "<img src=\"./images/equ3.png\" width=\"600\" />\n",
    "\n",
    "He finds that computing readmission probability using Equation (3) consistently outperforms predictions on each subsequence individually by 3–8%. This is because\n",
    "1. some subsequences (such as tokens corresponding to progress reports) do NOT contain information about readmission, whereas others do. The risk of readmission should be computed using subsequences that correlate with readmission risk, and **the effect of unimportant subsequences should be minimized**. This is accomplished by using the maximum probability over subsequences. \n",
    "2. Also noisy subsequences mislead the model and decrease performance. So they also include the average probability of readmission across subsequences. This leads to a trade-off between the mean and maximum probabilities of readmission in Equation (3).\n",
    "3. if there are a large number of subsequences for a patient with many clinical notes, there is a higher probability of having a noisy maximum probability of readmission. This means longer sequences may need to have a larger weight on the mean prediction. We include this weight as the n/c scaling factor, with c adjusting for patients with many clinical notes.\n",
    "Empirically, he found that c = 2 performs best on validation data.\n",
    "\n",
    "### Evaluation\n",
    "For validation and testing, 10% of the data is held out respectively, and 5-fold cross-validation is conducted. \n",
    "\n",
    "Each model is evaluated using three metrics:\n",
    "1. AUROC\n",
    "2. Area under the precision-recall curve\n",
    "3. Recall at precision of 80%: For the readmission task, false positives are important. To minimize the number of false positives and thus minimize the risk of alarm fatigue, he set the precision to 80% (in other words, 20% false positives out of the predicted positive class) and use the corresponding threshold to calculate recall. This leads to a clinically-relevant metric that enables us to build models that control the false positive rate. \n",
    "\n",
    "### Models\n",
    "* The training parameters are the entire encoder network, along with the classifier **`W`**\n",
    "* Note that the data labels are imbalanced: negative labels are subsampled to balance the positive readmit labels\n",
    "* ClinicalBert is trained for one epoch with batch size 4 and ee use the Adam optimizer learning rate 2 × 10−5\n",
    "*  The ClinicalBert model settings are the same as in Section 3.\n",
    "* The binary classifier is a linear layer of shape 768 × 1\n",
    "* The maximum sequence length supported by the model is set to 512, and the model is first trained using shorter sequences.\n",
    "\n",
    "<img src=\"./images/tab3.png\" width=\"600\" />\n",
    "\n",
    "Shows that ClinicalBERT outperforms it's competitors like Bag-of-words (Top 5000 TF-IDF words as features) and BiLSTM/Word2Vec in terms of precision and recall.\n",
    "\n",
    "###  Readmission Prediction With Early Clinical Notes\n",
    "Discharge summaries have predictive power for readmission. However, discharge summaries\n",
    "might be written after a patient has left the hospital. Therefore, discharge summaries are\n",
    "not actionable since doctors cannot intervene when a patient has left the hospital. Models\n",
    "that dynamically predict readmission in the early stages of a patient’s admission are relevant to clinicians.\n",
    "\n",
    "> **Note** that readmission predictions from a model are not actionable if a patient has been discharged. \n",
    "\n",
    "**24-48h**\n",
    "* In the MIMIC-III data, admission and discharge times are available, but clinical notes do not have timestamps. This is why the table headings show a range; this range shows the cutoff time for notes fed to the model from early on in a patient’s admission. For example, in the 24–48h column, the model may only take as input a patient’s notes up to 36h because of that patient’s specific admission time.\n",
    "\n",
    "**48-72h**\n",
    "* For the second set of readmission prediction experiments, a maximum of the first 48 or 72 hours of a patient’s notes are concatenated. These concatenated notes are used to predict readmission. Since we separate notes into subsequences of the same length, the training set consists of all subsequences within a maximum of 72 hours, and the model is tested given only available notes within the first 48 or 72 hours of a patient’s admission.\n",
    "* For testing 48 or 72-hour clinical note readmission prediction, patients that are discharged within 48 or 72 hours (respectively) are filtered out.\n",
    "\n",
    "### Interpretable predictions in ClinicalBert\n",
    "* ClinicalBert uses several self-attention mechanisms which can be used to inspect its predictions, by visualizing terms correlated with predictions of hospital readmission.\n",
    "    * For every clinical note input to ClinicalBert, each self-attention mechanism computes a distribution over every term in a sentence, given a query.\n",
    "    * **A high attention weight between a query and key token means the interaction between these tokens is predictive of readmission**.\n",
    "    *  In the ClinicalBert encoder, there are 144 self-attention mechanisms (or, 12 multi-head attention mechanisms for each of the 12 transformer encoders). \n",
    "  \n",
    "\n",
    "### Preprocessing\n",
    "ClinicalBert requires minimal preprocessing:\n",
    "1. First, words are converted to lowercase and\n",
    "2. line breaks are removed\n",
    "3. carriage returns are removed. \n",
    "4. De-identified the brackets \n",
    "5. remove special characters like ==, −−\n",
    "\n",
    "* The SpaCy sentence segmentation package is used to segment each note (Honnibal and Montani, 2017).\n",
    "    * Since clinical notes don't follow rigid standard language grammar, we find rule-based segmentation has better results than dependency parsing-based segmentation.\n",
    "    * Various segmentation signs that misguide rule-based segmentators are removed or replaced\n",
    "        * For example 1.2 would be removed\n",
    "        * M.D., dr. would be replaced with with MD, Dr\n",
    "    * Clinical notes can include various lab results and medications that also contain numerous rule-based separators, such as 20mg, p.o., q.d.. (where q.d. means one a day and q.o. means to take by mouth.  \n",
    "        *  To address this, segmentations that have less than 20 words are fused into the previous segmentation so that they are not singled out as different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Strings to Dates.\n",
    "When converting dates, it is safer to use a datetime format. \n",
    "Setting the errors = 'coerce' flag allows for missing dates \n",
    "but it sets it to NaT (not a datetime)  when the string doesn't match the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADMISSIONS table from AWS S3 bucket\n",
    "\n",
    "df_adm = pd.read_csv('ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADMISSIONS table\n",
    "# df_adm = pd.read_csv(\n",
    "#     '/Users/nwams/Documents/Machine Learning Projects/Predicting-Hospital-Readmission-using-NLP/ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm.ADMITTIME = pd.to_datetime(df_adm.ADMITTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df_adm.DISCHTIME = pd.to_datetime(df_adm.DISCHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df_adm.DEATHTIME = pd.to_datetime(df_adm.DEATHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the next Unplanned admission date for each patient (if it exists).\n",
    "I need to get the next admission date, if it exists.\n",
    "First I'll verify that the dates are in order.\n",
    "Then I'll use the shift() function to get the next admission date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = df_adm.sort_values(['SUBJECT_ID', 'ADMITTIME'])\n",
    "df_adm = df_adm.reset_index(drop=True)\n",
    "df_adm['NEXT_ADMITTIME'] = df_adm.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "df_adm['NEXT_ADMISSION_TYPE'] = df_adm.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to predict unplanned re-admissions I will drop (filter out) any future admissions that are ELECTIVE \n",
    "so that only EMERGENCY re-admissions are measured.\n",
    "For rows with 'elective' admissions, replace it with NaT and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df_adm.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
    "df_adm.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
    "df_adm.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's safer to sort right before the fill incase something I did above changed the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = df_adm.sort_values(['SUBJECT_ID','ADMITTIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backfill in the values that I removed. So copy the ADMITTIME from the last emergency \n",
    "and paste it in the NEXT_ADMITTIME for the previous emergency. \n",
    "So I am effectively ignoring/skipping the ELECTIVE admission row completely. \n",
    "Doing this will allow me to calculate the days until the next admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back fill. This will take a little while.\n",
    "df_adm[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = df_adm.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
    "\n",
    "# Calculate days until next admission\n",
    "df_adm['DAYS_NEXT_ADMIT'] = (df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME).dt.total_seconds()/(24*60*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NEWBORN admissions\n",
    "According to the MIMIC site \"Newborn indicates that the HADM_ID pertains to the patient's birth.\"\n",
    "\n",
    "I will remove all NEWBORN admission types because in this project I'm not interested in studying births — my primary \n",
    "interest is EMERGENCY and URGENT admissions.\n",
    "I will remove all admissions that have a DEATHTIME because in this project I'm studying re-admissions, not mortality. \n",
    "And a patient who died cannot be re-admitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = df_adm.loc[df_adm.ADMISSION_TYPE != 'NEWBORN']\n",
    "df_adm = df_adm.loc[df_adm.DEATHTIME.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Output Label\n",
    "For this problem, we are going to classify if a patient will be admitted in the next 30 days. \n",
    "Therefore, we need to create a variable with the output label (1 = readmitted, 0 = not readmitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm['OUTPUT_LABEL'] = (df_adm.DAYS_NEXT_ADMIT < 30).astype('int')\n",
    "df_adm['DURATION'] = (df_adm['DISCHTIME']-df_adm['ADMITTIME']).dt.total_seconds()/(24*60*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NOTEEVENTS Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load ADMISSIONS table from AWS S3 bucket\n",
    "\n",
    "df_notes = pd.read_csv('NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by subject_ID, HAD_ID then CHARTDATE\n",
    "df_notes = df_notes.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\n",
    "# Merge notes table to admissions table\n",
    "df_adm_notes = pd.merge(df_adm[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME','OUTPUT_LABEL','DURATION']],\n",
    "                        df_notes[['SUBJECT_ID','HADM_ID','CHARTDATE','TEXT','CATEGORY']],\n",
    "                        on = ['SUBJECT_ID','HADM_ID'],\n",
    "                        how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Grab date only, not the time\n",
    "df_adm_notes.ADMITTIME_C = df_adm_notes.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n",
    "\n",
    "df_adm_notes['ADMITTIME_C'] = pd.to_datetime(df_adm_notes.ADMITTIME_C, format = '%Y-%m-%d', errors = 'coerce')\n",
    "df_adm_notes['CHARTDATE'] = pd.to_datetime(df_adm_notes.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather Discharge Summaries Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather Discharge Summaries Only\n",
    "df_discharge = df_adm_notes[df_adm_notes['CATEGORY'] == 'Discharge summary']\n",
    "# multiple discharge summary for one admission -> after examination -> replicated summary -> replace with the last one\n",
    "df_discharge = (df_discharge.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
    "df_discharge=df_discharge[df_discharge['TEXT'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Less than n days on admission notes (Early notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_n_days_data(df_adm_notes, n):\n",
    "    df_less_n = df_adm_notes[\n",
    "        ((df_adm_notes['CHARTDATE'] - df_adm_notes['ADMITTIME_C']).dt.total_seconds() / (24 * 60 * 60)) < n]\n",
    "    df_less_n = df_less_n[df_less_n['TEXT'].notnull()]\n",
    "    # concatenate first\n",
    "    df_concat = pd.DataFrame(df_less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n",
    "    df_concat['OUTPUT_LABEL'] = df_concat['HADM_ID'].apply(\n",
    "        lambda x: df_less_n[df_less_n['HADM_ID'] == x].OUTPUT_LABEL.values[0])\n",
    "    \n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less_2 = less_n_days_data(df_adm_notes, 2)\n",
    "df_less_3 = less_n_days_data(df_adm_notes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess1(x):\n",
    "    y = re.sub('\\\\[(.*?)\\\\]', '', x)  # remove de-identified brackets\n",
    "    y = re.sub('[0-9]+\\.', '', y)  # remove 1.2. since the segmenter segments based on this\n",
    "    y = re.sub('dr\\.', 'doctor', y)\n",
    "    y = re.sub('m\\.d\\.', 'md', y)\n",
    "    y = re.sub('admission date:', '', y)\n",
    "    y = re.sub('discharge date:', '', y)\n",
    "    y = re.sub('--|__|==', '', y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df_less_n):\n",
    "    df_less_n['TEXT'] = df_less_n['TEXT'].fillna(' ')\n",
    "    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\n', ' ')\n",
    "    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\r', ' ')\n",
    "    df_less_n['TEXT'] = df_less_n['TEXT'].apply(str.strip)\n",
    "    df_less_n['TEXT'] = df_less_n['TEXT'].str.lower()\n",
    "\n",
    "    df_less_n['TEXT'] = df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
    "\n",
    "    # to get 318 words chunks for readmission tasks\n",
    "    df_len = len(df_less_n)\n",
    "    want = pd.DataFrame({'ID': [], 'TEXT': [], 'Label': []})\n",
    "    for i in tqdm(range(df_len)):\n",
    "        x = df_less_n.TEXT.iloc[i].split()\n",
    "        n = int(len(x) / 318)\n",
    "        for j in range(n):\n",
    "            want = want.append({'TEXT': ' '.join(x[j * 318:(j + 1) * 318]), 'Label': df_less_n.OUTPUT_LABEL.iloc[i],\n",
    "                                'ID': df_less_n.HADM_ID.iloc[i]}, ignore_index=True)\n",
    "        if len(x) % 318 > 10:\n",
    "            want = want.append({'TEXT': ' '.join(x[-(len(x) % 318):]), 'Label': df_less_n.OUTPUT_LABEL.iloc[i],\n",
    "                                'ID': df_less_n.HADM_ID.iloc[i]}, ignore_index=True)\n",
    "\n",
    "    return want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing below for the Discharge, 2-Day and 3-Day stays took about 6.5 hours on my local machine (discharge=2.5hrs, 2-day=1.5 hrs and 3-day=2.5 hrs). \n",
    "\n",
    "Uncomment the lines below (I've commented it out since I've already run preprocessing and pickled the files). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_discharge = preprocessing(df_discharge)\n",
    "#df_less_2 = preprocessing(df_less_2)\n",
    "df_less_3 = preprocessing(df_less_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pickle it for later use. Uncomment the code below to pickle your files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_discharge.to_pickle(\"./pickle/df_discharge.pkl\")\n",
    "#df_less_2.to_pickle(\"./pickle/df_less_2.pkl\")\n",
    "#df_less_3.to_pickle(\"./pickle/df_less_3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pickled files, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_discharge = pd.read_pickle('./pickle/df_discharge.pkl')\n",
    "#df_less_2 = pd.read_pickle('./pickle/df_less_2.pkl')\n",
    "df_less_3 = pd.read_pickle('./pickle/df_less_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216954, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_discharge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277443, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_less_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385724, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_less_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discharge has 216,954 rows. \n",
    "\n",
    "2-Day has 277,443 rows.\n",
    "\n",
    "3-Day has 385,724 rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "472px",
    "left": "506px",
    "right": "20px",
    "top": "120px",
    "width": "742px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
