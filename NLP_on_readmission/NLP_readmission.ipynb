{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Thanks to Nwamaka's medium post(https://medium.com/nwamaka-imasogie/clinicalbert-using-deep-learning-transformer-model-to-predict-hospital-readmission-c82ff0e4bb03), which offers the data preprocessing stage as well as some guidance on classification. The following project will read the data from the pre-processed stage, and perform the machine learning techniques on Readmission prediction.\n",
    "\n",
    "This part is a good supplement for real-time text prediction. We can get strucutured data immediately, but it has a lag(The hospital will not count the structured data until the end of the day!). This is the reason why NLP could help on real-time predicting.\n",
    "\n",
    "We will mainly focus on:\n",
    "\n",
    "1. Simple convertion of word vectors\n",
    "\n",
    "2. Data Exploration and unbalanced data issue\n",
    "\n",
    "3. Applying more complex model to improve the roc_auc score\n",
    "\n",
    "We intentionaly kept the code not that consise (you might see repeated use of code). The reason is that we left an analytic procedure to you\n",
    "\n",
    "The result might seems not quite good since both unbalanced data effect and there is few NLP task exploration on this dataset. Even the original author just got 0.76 ROC_AUC score. We want to explore NLP method rather than beating anyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read the pre-processed data and EDA\n",
    "\n",
    "1. in this section, we read the data from the what the authoer have preprocessed (3 days readmission except balancing stage).\n",
    "2. we will do data exploaration before applying machine learning model.\n",
    "\n",
    "Imagine that, you only have the text data, nothing else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('./pickle/df_less_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>355589</td>\n",
       "      <td>355589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>30135</td>\n",
       "      <td>30135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID    TEXT\n",
       "Label                \n",
       "0.0    355589  355589\n",
       "1.0     30135   30135"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Label').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this is an unbalanced dataset with 1:12 of positive against negative samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the maximum length is ... 3517\n",
      "the minimum length is ... 43\n"
     ]
    }
   ],
   "source": [
    "df['text_len']=df.TEXT.apply(lambda x:len(x))\n",
    "print('the maximum length is ...', df.text_len.max())\n",
    "print('the minimum length is ...', df.text_len.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to make sure during word embedding(above is just the string length, not how many words), we wont suffer a very long sentence.In addition, we want to know the range of the possible length of our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID          0\n",
       "TEXT        0\n",
       "Label       0\n",
       "text_len    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! we have no NaN data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets split our data to train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,['TEXT','text_len']],df['Label'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>294746</td>\n",
       "      <td>explanation given acute change in imaging. len...</td>\n",
       "      <td>2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33668</td>\n",
       "      <td>sinus tachycardia. probable left ventricular h...</td>\n",
       "      <td>2195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197820</td>\n",
       "      <td>course: 66f with afib on coumadin, remote h/o ...</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90785</td>\n",
       "      <td>cholycystitis with common bile duct stones wit...</td>\n",
       "      <td>1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34373</td>\n",
       "      <td>full code (was noted to be dnr/dni prior to ad...</td>\n",
       "      <td>1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359783</td>\n",
       "      <td>ivl. piv x site wnl. patent. social: no family...</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358083</td>\n",
       "      <td>ml d5w acetaminophen aspirin ec aspirin atorva...</td>\n",
       "      <td>1926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152315</td>\n",
       "      <td>man with left renal mass and h/o subdural hemm...</td>\n",
       "      <td>1798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117952</td>\n",
       "      <td>icu care nutrition: glycemic control: lines: 1...</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305711</td>\n",
       "      <td>7:20 p 1//11/006 1:23 p 1:20 p 11:20 p 4:20 p ...</td>\n",
       "      <td>2076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270006 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     TEXT  text_len\n",
       "294746  explanation given acute change in imaging. len...      2112\n",
       "33668   sinus tachycardia. probable left ventricular h...      2195\n",
       "197820  course: 66f with afib on coumadin, remote h/o ...      1849\n",
       "90785   cholycystitis with common bile duct stones wit...      1883\n",
       "34373   full code (was noted to be dnr/dni prior to ad...      1935\n",
       "...                                                   ...       ...\n",
       "359783  ivl. piv x site wnl. patent. social: no family...       224\n",
       "358083  ml d5w acetaminophen aspirin ec aspirin atorva...      1926\n",
       "152315  man with left renal mass and h/o subdural hemm...      1798\n",
       "117952  icu care nutrition: glycemic control: lines: 1...      1867\n",
       "305711  7:20 p 1//11/006 1:23 p 1:20 p 11:20 p 4:20 p ...      2076\n",
       "\n",
       "[270006 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.DataFrame(X_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of pos/neg in training set  0.08377385754711301\n",
      "the ratio of pos/neg in test set  0.08702350310932422\n"
     ]
    }
   ],
   "source": [
    "print('the ratio of pos/neg in training set ',sum(y_train==1)/sum(y_train==0))\n",
    "print('the ratio of pos/neg in test set ',sum(y_test==1)/sum(y_test==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do NLP processing, we also want to know weather the basic statistic information is useful to distinguish the readmission. For example, the length of the \"TEXT\" column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cd205ed4c8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "info=df_train.groupby('text_len',as_index=0).count()\n",
    "\n",
    "plt.plot(info.text_len,info.TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of text_len against how many Labels seem very normal. We wonder if the pos/neg labels also follow such distribution. Hence, lets do a very simple logistic regression by using only the \"text_len\" as the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the ratio of pos/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of pos/neg is:  0.08377385754711301\n"
     ]
    }
   ],
   "source": [
    "print('the ratio of pos/neg is: ',sum(y_train==1)/sum(y_train==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see since the data is imbalanced 1:12.5, our grid search should from 1:5 to 1:15.\n",
    "\n",
    "Also, we will use GridSearch to find the best weights.\n",
    "\n",
    "According to the metrics of binary classification. We will use ROC/AUC curve as the evaluation.The benefit is that we could avoid bias on recall/precision trade-off since the final result is based upon the ROC's area.\n",
    "\n",
    "To generate the K-fold, we use RepeatedStratifiedKFold because it could generate sub-group as the original dataset distribution.\n",
    "\n",
    "Finally, we will use the best weights to fit the model.\n",
    "\n",
    "**However, below is a demonstration example to show you how the unbalanced model might cheat you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.512811 using {'class_weight': {0: 1, 1: 2}}\n",
      "0.487189 (0.004305) with: {'class_weight': {0: 1, 1: 1}}\n",
      "0.512811 (0.004305) with: {'class_weight': {0: 1, 1: 2}}\n",
      "0.487189 (0.004305) with: {'class_weight': {0: 1, 1: 3}}\n",
      "0.512811 (0.004305) with: {'class_weight': {0: 1, 1: 4}}\n",
      "0.487189 (0.004305) with: {'class_weight': {0: 1, 1: 5}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold # this gurantee each split is almost follow the distribution of the original dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X=df_train['text_len'].values.reshape(-1,1)\n",
    "y=y_train.values\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "grid_val=[dict({0:1,1:j}) for j in range(1,6) ]\n",
    "param_grid = dict(class_weight=grid_val)\n",
    "cv = RepeatedStratifiedKFold(n_splits=6)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
    "\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cls=LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cls.predict(X)) #see， we have make all prediction to negative label！\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y, cls.predict(X)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn  as  sns\n",
    "\n",
    "cm = np.array([[fn,tp],[tn,fp]])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm,annot=True,linewidths=1, fmt = 'd')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model reports TP+FP=0, which means that all instances were predicted as negative.\n",
    "\n",
    "In addition, it indicates that it is impossible to predict the label only on sentence length!\n",
    "\n",
    "Now, we can see that for unbalanced data, we must consider using AUC curve as the metric, instead of either precision/recall rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: First Attempt and Problem Finding\n",
    "\n",
    "1. In this part, we are going to use pre-trained word vector from spacy library.\n",
    "2. The kernel of NLP job is to think about \"word_embedding\". The major methods to convert one word or sentence to a vector are: 'one-hot encoding' 'statistical_based embedding',and 'word_embedding' : \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    one-hot encoding is sparse and orthogonal between each word, also huge cost on memory space\n",
    "    \n",
    "    word_embedding is an dense representation of words involving the pre-trained information. You could simply think word_embedding is a universal method like dimensionality reduction, signal capture or compression.\n",
    "    \n",
    "    word_embedding includes statis and dynamic types. For example, one word maps to one unique vector is called static embedding(Word2vec). Whereas one word maps to one vector based upon some given contextual texts called dynamic embedding(Bert).\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "In this project, we will focus on  static \"word_embedding\", which is 'Glove'. By the time limitation, We will only focus on static word embedding. Nwamaka already implemented the clinical bert pipeline, we do not want to just simply re-run her code and instead we want to explore the project on our own knowledge and interests.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1: We use the pre-trained model from Spacy library to get the word embedding:\n",
    "\n",
    "\n",
    "Here we use \"en_ner_bionlp13cg_md\" model, which is a very complex model for Named Entity Recognition(NER) task. NER task requires a supervised learning which involves TAGs on each word in a sentence. For example, \"en_ner_bionlp13cg_md\" model's main job is to recognize the possible biological/clinical/medical names in the sentences. We will call the function to get the pre-trained word vector on that task.\n",
    "\n",
    "*please see https://allenai.github.io/scispacy/ for detailed introduction and installation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_sm   \n",
    "from spacy import displacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "\n",
    "\n",
    "import en_ner_bionlp13cg_md\n",
    "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(df_train.TEXT[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention, you need to close the displacy.serve by yourself to continue running codes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">sinus rhythm prolonged qt interval is nonspecific but clinical correlation is suggested no previous tracing available for comparison chief complaint: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gib/hotn i\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GENE_OR_GENE_PRODUCT</span>\n",
       "</mark>\n",
       " saw and examined the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    patient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM</span>\n",
       "</mark>\n",
       ", and was physically present with the icu resident for key portions of the services provided. i agree with his / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    her\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM</span>\n",
       "</mark>\n",
       " note above, including assessment and plan. hpi: 62 yo \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    man\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hep c\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GENE_OR_GENE_PRODUCT</span>\n",
       "</mark>\n",
       " cirrhosis presents with one episode of hematemesis. in ed vs were stable. initial hct was 26, (prior one 1 month ago was 8). transferred to micu where became hypotensive, started on \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    dopamine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIMPLE_CHEMICAL</span>\n",
       "</mark>\n",
       ". repeat hct came back at unable to get ngt. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    central line\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CELL</span>\n",
       "</mark>\n",
       " placed. of note u/s in ed showed no \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ascites\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       ". allergies: no known drug allergies last dose of antibiotics: infusions: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    octreotide\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIMPLE_CHEMICAL</span>\n",
       "</mark>\n",
       " - 48 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mcg/hour\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GENE_OR_GENE_PRODUCT</span>\n",
       "</mark>\n",
       " other icu medications: other medications: past medical history: family history: social history: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hep c\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GENE_OR_GENE_PRODUCT</span>\n",
       "</mark>\n",
       " cirrhosis meds at home: aldactone lasix lisinopril no \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cirrhosis + colon polyps\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CANCER</span>\n",
       "</mark>\n",
       " occupation: drugs: none \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tobacco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM</span>\n",
       "</mark>\n",
       ": 20 per day. alcohol: 1 per week. other: lives alone review of systems: constitutional: no(t) fever cardiovascular: no(t) \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM_SUBDIVISION</span>\n",
       "</mark>\n",
       " pain respiratory: no(t) cough, no(t) dyspnea gastrointestinal: no(t) \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    abdominal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       " pain, emesis \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    genitourinary\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       ": no(t) dysuria integumentary (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    skin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       "): no(t) jaundice endocrine: no(t) hyperglycemia \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    heme\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GENE_OR_GENE_PRODUCT</span>\n",
       "</mark>\n",
       " / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    lymph\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MULTI-TISSUE_STRUCTURE</span>\n",
       "</mark>\n",
       ": anemia neurologic: no(t) headache psychiatric / sleep: no(t) agitated, somnolent allergy / immunology: no(t) immunocompromised flowsheet data as of 08:47 pm vital signs hemodynamic monitoring fluid balance 24 hours since 12 am hr: 69 (69 - 73) bpm bp: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    77/45(47\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIMPLE_CHEMICAL</span>\n",
       "</mark>\n",
       ") {77/36(47) - 90/47(57)} mmhg rr: 12 (11 - 16) insp/min spo2: 97% \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    heart\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       " rhythm: sr (sinus rhythm) total in: 2,756 ml po: tf: ivf: 1,037 ml \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    blood\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM_SUBSTANCE</span>\n",
       "</mark>\n",
       " products: 719 ml total out: 0 ml 150 ml \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    urine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM_SUBSTANCE</span>\n",
       "</mark>\n",
       ": 150 ml ng: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    stool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANISM_SUBDIVISION</span>\n",
       "</mark>\n",
       ": drains: balance: 0 ml \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2,606\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIMPLE_CHEMICAL</span>\n",
       "</mark>\n",
       " ml respiratory o2 delivery device: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nasal cannula\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIMPLE_CHEMICAL</span>\n",
       "</mark>\n",
       " spo2: 97% abg: 37/29//16/-6 physical examination general appearance: well nourished, no acute distress \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    eyes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       " / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    conjunctiva\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       ": perrl, no icterus head, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ears\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PATHOLOGICAL_FORMATION</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nose\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    throat\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGAN</span>\n",
       "</mark>\n",
       ": normocephalic lymphatic:</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the entity has been already perfect marked by using this model.Hence, we can use the word vectors directly from this model to convert each sentence to a numerical vector.\n",
    "\n",
    "We first make some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>2742</td>\n",
       "      <td>2742</td>\n",
       "      <td>2742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  TEXT  text_len\n",
       "Label                      \n",
       "0.0    2742  2742      2742\n",
       "1.0     258   258       258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample=df.sample(3000,random_state=0)\n",
    "df_sample.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "count=0\n",
    "begin=time.time()\n",
    "def get_ner_wv(text):\n",
    "    global count\n",
    "    global begin\n",
    "    count+=1\n",
    "    if count%1000==0:\n",
    "        print('now processing.... ',count,' time cost...... ',time.time()-begin,' s')\n",
    "    doc=nlp(text)\n",
    "    return doc.vector\n",
    "\n",
    "#df_sample=df_train.sample(3000)\n",
    "#df_train['ner_wv']=df_train.TEXT.apply(get_ner_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing....  1000  time cost......  46.94900703430176  s\n",
      "now processing....  2000  time cost......  91.73923444747925  s\n",
      "now processing....  3000  time cost......  136.30502367019653  s\n"
     ]
    }
   ],
   "source": [
    "df_sample['word2vec']=df_sample.TEXT.apply(get_ner_wv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_len</th>\n",
       "      <th>word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27280</td>\n",
       "      <td>107136.0</td>\n",
       "      <td>remove picc and send tip...wean neo to map &gt;.....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1941</td>\n",
       "      <td>[0.029284183, 0.014483439, 0.06752385, 0.00371...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241706</td>\n",
       "      <td>162488.0</td>\n",
       "      <td>of readiness for learning action: mi teaching ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>[-0.0017557873, 0.046223473, 0.10897355, 0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199909</td>\n",
       "      <td>151376.0</td>\n",
       "      <td>is normal (lvef&gt;55%). right ventricle: right v...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2256</td>\n",
       "      <td>[0.009158471, 0.06597229, 0.0048928526, -0.035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175220</td>\n",
       "      <td>145096.0</td>\n",
       "      <td>represents a fat pad, though a loculated anter...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2163</td>\n",
       "      <td>[-0.006347567, 0.075405955, 0.046496622, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163835</td>\n",
       "      <td>141915.0</td>\n",
       "      <td>was briefly placed on cpap. no abg obtained. v...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>[-0.0011737325, 0.027778277, 0.1136065, -0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290671</td>\n",
       "      <td>175091.0</td>\n",
       "      <td>supraventricular tachycardia left axis deviati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>[0.023522438, 0.047339242, 0.056796785, 0.0157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152991</td>\n",
       "      <td>139055.0</td>\n",
       "      <td>16 (11 - 24) insp/min spo2: 93% heart rhythm: ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1763</td>\n",
       "      <td>[0.055983152, 0.03939668, 0.09662792, 0.032728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384581</td>\n",
       "      <td>199679.0</td>\n",
       "      <td>sinus tachycardia, rate left atrial abnormalit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2211</td>\n",
       "      <td>[-0.01792933, 0.046701074, 0.086492136, 0.0291...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240217</td>\n",
       "      <td>162171.0</td>\n",
       "      <td>seen. pericardium: there is a small to moderat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877</td>\n",
       "      <td>[0.0026870752, 0.12834427, 0.022967158, 0.0031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300485</td>\n",
       "      <td>177612.0</td>\n",
       "      <td>expected. the fourth ventricle remains obliter...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>[0.022608442, 0.01802392, 0.06332687, 0.006647...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               TEXT  Label  \\\n",
       "27280   107136.0  remove picc and send tip...wean neo to map >.....    1.0   \n",
       "241706  162488.0  of readiness for learning action: mi teaching ...    0.0   \n",
       "199909  151376.0  is normal (lvef>55%). right ventricle: right v...    0.0   \n",
       "175220  145096.0  represents a fat pad, though a loculated anter...    0.0   \n",
       "163835  141915.0  was briefly placed on cpap. no abg obtained. v...    0.0   \n",
       "...          ...                                                ...    ...   \n",
       "290671  175091.0  supraventricular tachycardia left axis deviati...    0.0   \n",
       "152991  139055.0  16 (11 - 24) insp/min spo2: 93% heart rhythm: ...    0.0   \n",
       "384581  199679.0  sinus tachycardia, rate left atrial abnormalit...    0.0   \n",
       "240217  162171.0  seen. pericardium: there is a small to moderat...    0.0   \n",
       "300485  177612.0  expected. the fourth ventricle remains obliter...    0.0   \n",
       "\n",
       "        text_len                                           word2vec  \n",
       "27280       1941  [0.029284183, 0.014483439, 0.06752385, 0.00371...  \n",
       "241706      2016  [-0.0017557873, 0.046223473, 0.10897355, 0.027...  \n",
       "199909      2256  [0.009158471, 0.06597229, 0.0048928526, -0.035...  \n",
       "175220      2163  [-0.006347567, 0.075405955, 0.046496622, -0.00...  \n",
       "163835      2016  [-0.0011737325, 0.027778277, 0.1136065, -0.004...  \n",
       "...          ...                                                ...  \n",
       "290671      2077  [0.023522438, 0.047339242, 0.056796785, 0.0157...  \n",
       "152991      1763  [0.055983152, 0.03939668, 0.09662792, 0.032728...  \n",
       "384581      2211  [-0.01792933, 0.046701074, 0.086492136, 0.0291...  \n",
       "240217       877  [0.0026870752, 0.12834427, 0.022967158, 0.0031...  \n",
       "300485      2139  [0.022608442, 0.01802392, 0.06332687, 0.006647...  \n",
       "\n",
       "[3000 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_NLP(X,Y,title):\n",
    "    \n",
    "    print('this is for....',title,'\\n\\n\\n*********************')\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "    grid_val=[dict({0:1,1:j}) for j in range(1,5) ]\n",
    "    param_grid = dict(class_weight=grid_val)\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=6)\n",
    "    # define grid search\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
    "    # execute the grid search\n",
    "    grid_result = grid.fit(X, Y)\n",
    "    # report the best configuration\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    # report all configurations\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "    print('\\n\\n\\n*********************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is for.... word vector \n",
      "\n",
      "\n",
      "*********************\n",
      "Best: 0.566964 using {'class_weight': {0: 1, 1: 4}}\n",
      "0.560954 (0.039526) with: {'class_weight': {0: 1, 1: 1}}\n",
      "0.563428 (0.039004) with: {'class_weight': {0: 1, 1: 2}}\n",
      "0.565391 (0.039223) with: {'class_weight': {0: 1, 1: 3}}\n",
      "0.566964 (0.039450) with: {'class_weight': {0: 1, 1: 4}}\n",
      "\n",
      "\n",
      "\n",
      "*********************\n",
      "this is for.... number \n",
      "\n",
      "\n",
      "*********************\n",
      "Best: 0.539913 using {'class_weight': {0: 1, 1: 3}}\n",
      "0.525201 (0.059190) with: {'class_weight': {0: 1, 1: 1}}\n",
      "0.537130 (0.052535) with: {'class_weight': {0: 1, 1: 2}}\n",
      "0.539913 (0.050453) with: {'class_weight': {0: 1, 1: 3}}\n",
      "0.537130 (0.052535) with: {'class_weight': {0: 1, 1: 4}}\n",
      "\n",
      "\n",
      "\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_wv= df_sample.word2vec.apply(pd.Series).values\n",
    "X_num=df_sample['text_len'].values.reshape(-1,1)\n",
    "Y=df_sample.Label\n",
    "\n",
    "\n",
    "logit_NLP(X_wv,Y,'word vector')\n",
    "logit_NLP(X_num,Y,'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4956043956043956"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_wv[1:2500], Y[1:2500])\n",
    "\n",
    "roc_auc_score(Y[2500:],clf.predict(X_wv[2500:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.484981684981685"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_wv[1:2500], Y[1:2500])\n",
    "roc_auc_score(Y[2500:],clf.predict(X_wv[2500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 726\n"
     ]
    }
   ],
   "source": [
    "pred=clf.predict(X_wv[2200:])\n",
    "print(sum(pred==1),sum(pred==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we can get two conclusions:\n",
    "    \n",
    "1. We are still facing unbalanced dataset and majority of machine learning model are very sensitive to unbalanced data!\n",
    "\n",
    "2. The count-based information is narrow, we need to expand word to vectors to represent more meanings.\n",
    "\n",
    "*So, do we have some solutions to solve such unbalanced data and applied more complex NLP model？*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Solving the problem\n",
    "\n",
    "Unfortunately, there is no end-to-end solution to solve such unbalanced data problem. All methods are facing challenges when the environment changes. That is the reason why the company still requires data team write a lot of SQL to deeply dig the data rather than auto machine learning. \n",
    "\n",
    "Right now, the most popular solutions are still: upsampling, downsampling, modified class weight and SMOTE. SMOTE works in feature space. It means that the output of SMOTE is not a synthetic data which is a real representative of a text inside its feature space.On one side SMOTE works with KNN and on the other hand, feature spaces for NLP problem are dramatically huge. KNN will easily fail in those huge dimensions.\n",
    "\n",
    "Here we already tried adjustment on weights. It does not work well. For SMOTE method, it is to create artificial data point by using interpolation method. You may still think it is a more smoothy method to change the distribution of data, but the trade-off is that it is time-costly.\n",
    "\n",
    "Now, I want to use the fastest method by rebalancing the data from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "split=int(df.shape[0]*0.75)\n",
    "\n",
    "seed=0\n",
    "\n",
    "kaka=df.iloc[0:300000,:].sample(100000)\n",
    "\n",
    "\n",
    "g = df.groupby('Label')\n",
    "final=g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "final.index=range(final.shape[0])\n",
    "\n",
    "final=final.sample(30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=final[0:20000]\n",
    "test_df=final[20000:] \n",
    "\n",
    "#we must make sure there is no any overlap dataset between train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>15061</td>\n",
       "      <td>15061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>14939</td>\n",
       "      <td>14939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   TEXT\n",
       "Label              \n",
       "0.0    15061  15061\n",
       "1.0    14939  14939"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>10095</td>\n",
       "      <td>10095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>9905</td>\n",
       "      <td>9905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   TEXT\n",
       "Label              \n",
       "0.0    10095  10095\n",
       "1.0     9905   9905"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>4966</td>\n",
       "      <td>4966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>5034</td>\n",
       "      <td>5034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  TEXT\n",
       "Label            \n",
       "0.0    4966  4966\n",
       "1.0    5034  5034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, train dataset holds a sound distribution with 1:1. This could help the model to make classification. But the test dataset should follow the original distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv')\n",
    "test_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: NLP and experienment\n",
    "\n",
    "In this stage, we will explore experienment modern techniques in NLP. Performing word embedding+ contrcution of model in Pytorch.\n",
    "There are diverse word embedding method possible to be selected, and each of them might bring different performance. The code below was modified from\n",
    "https://www.kaggle.com/swarnabha/pytorch-text-classification-torchtext-lstm\n",
    "\n",
    "\n",
    "There are several things to be noticed. The RNN model does the same thing like general NN model except it adds one more step to process the sequantial data. For example, it adds a for loop to pass the sequential data until the last output. This property forces the embedding dimensionality to be the same, so the sentences in each batch must keep the same length. We will talk more details as we go there. Before we go, these links are very helpful to read for a better understanding on NLP processing. We will use torchtext to help us maintain the structure of the data. \n",
    "\n",
    "Word Embedding with real example: https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598\n",
    "\n",
    "RNN numpy implementation: https://victorzhou.com/blog/intro-to-rnns/\n",
    "\n",
    "\n",
    "\n",
    "What we did different?\n",
    "\n",
    "1. modified the word embedding methods and parameters\n",
    "\n",
    "2. changed the structure of models\n",
    "\n",
    "3. added ROC_AUC metrics and test dataset within evaluation stage\n",
    "\n",
    "4. picked better hyper-parameters\n",
    "\n",
    "First, lets show a demonstration of how pytorch get the corresponding word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3128, -1.0663],\n",
       "        [ 0.9047, -1.2541],\n",
       "        [-0.5348, -0.4384],\n",
       "        [ 0.5261, -1.1570],\n",
       "        [-0.8455,  0.6700],\n",
       "        [-0.4973,  0.3867],\n",
       "        [ 0.0025,  0.9776],\n",
       "        [-0.2608,  0.4132],\n",
       "        [ 0.8033,  0.1650],\n",
       "        [-0.0236,  0.1517]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=nn.Embedding(10,2)\n",
    "emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9047, -1.2541],\n",
       "        [-0.5348, -0.4384],\n",
       "        [-0.8455,  0.6700],\n",
       "        [ 0.0025,  0.9776]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "emb(torch.tensor([1,2,4,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.embedding does nothing but provide a look-up table for us to check the corresponding word's vector by given the index\n",
    "We will pass the pre-trained embedding to that field when initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4)\n",
      "do train\n",
      "do valid\n",
      "do test\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(train.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_df, valid_df = train_test_split(train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    # return a list of <class 'spacy.tokens.token.Token'>\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "TEXT = data.Field(tokenize = tokenizer, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, fields, is_test=False, **kwargs):\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.Label if not is_test else None\n",
    "            text = row.TEXT\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        data_field = fields\n",
    "\n",
    "        if train_df is not None:\n",
    "            print('do train')\n",
    "            train_data = cls(train_df.copy(), data_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            print('do valid')\n",
    "            val_data = cls(val_df.copy(), data_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            print('do test')\n",
    "            test_data = cls(test_df.copy(), data_field, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n",
    "    \n",
    "fields = [('text',TEXT), ('label',LABEL)]\n",
    "\n",
    "train_ds, val_ds ,test_ds= DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df,test_df=test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT.build_vocab(train_ds, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = 'glove.6B.50d',\n",
    "                 unk_init = torch.Tensor.zero_)\n",
    "\n",
    "LABEL.build_vocab(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an essential step which load the pretrained word embedding. The Glove model, is a static model which means we can immediately load a word vector just by calling .vector(word). Static embedding means one-to-one which implies there will be no association with the contextual words. For example: \"I ate apple\", \"Apple stock will split with ratio 2:1 in 2021, reported by Yahoo Finance\" will return the same vector for word apple. For more details, please check the links at the top.\n",
    "\n",
    "Below, we will set the hyper-parameter for classification model. The reason we use RNN(LSTM) model is because we think there are some relationship between the words in each sentence. For example, a bone disease usually requires a radiology as the following precedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002, 50])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
      "        ...,\n",
      "        [-0.4793, -0.0630, -0.4590,  ...,  0.9031,  0.2111,  0.1730],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2764, -0.1602,  0.0258,  ...,  0.0723, -0.0696, -0.1483]])\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.42%\n",
      "\t Val. Acc: 48.64%\n",
      "\t Val. ROC_AUC: 52.54%\n",
      "\tTrain Loss: 0.676 | Train Acc: 58.22%\n",
      "\t Val. Acc: 54.17%\n",
      "\t Val. ROC_AUC: 56.85%\n",
      "\tTrain Loss: 0.593 | Train Acc: 68.72%\n",
      "\t Val. Acc: 56.39%\n",
      "\t Val. ROC_AUC: 58.92%\n",
      "\tTrain Loss: 0.469 | Train Acc: 77.72%\n",
      "\t Val. Acc: 56.39%\n",
      "\t Val. ROC_AUC: 59.21%\n",
      "\tTrain Loss: 0.349 | Train Acc: 84.88%\n",
      "\t Val. Acc: 57.22%\n",
      "\t Val. ROC_AUC: 59.36%\n",
      "\tTrain Loss: 0.250 | Train Acc: 89.40%\n",
      "\t Val. Acc: 55.93%\n",
      "\t Val. ROC_AUC: 58.59%\n",
      "\tTrain Loss: 0.184 | Train Acc: 92.38%\n",
      "\t Val. Acc: 55.95%\n",
      "\t Val. ROC_AUC: 58.38%\n",
      "\tTrain Loss: 0.131 | Train Acc: 94.85%\n",
      "\t Val. Acc: 56.90%\n",
      "\t Val. ROC_AUC: 59.09%\n",
      "\tTrain Loss: 0.099 | Train Acc: 96.15%\n",
      "\t Val. Acc: 56.55%\n",
      "\t Val. ROC_AUC: 58.79%\n",
      "\tTrain Loss: 0.080 | Train Acc: 97.06%\n",
      "\t Val. Acc: 56.47%\n",
      "\t Val. ROC_AUC: 59.06%\n",
      "time:413.745\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "\n",
    "train_iterator, valid_iterator,test_iterator = data.BucketIterator.splits(\n",
    "    (train_ds, val_ds,test_ds), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding\n",
    "\n",
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        output = self.fc1(hidden)\n",
    "        output = self.fc2(output)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return output\n",
    "    \n",
    "model = LSTM_net(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)\n",
    "\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    rounded_preds = (torch.sigmoid(preds)>0.32).float()\n",
    "    #print(rounded_preds)\n",
    "    #print(roc_auc_score(y.cpu().data.numpy(),rounded_preds.cpu().data.numpy()))\n",
    "\n",
    "    \n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc,rounded_preds,torch.sigmoid(preds)\n",
    "\n",
    "\n",
    "def train(model, iterator):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    pred_collect=torch.empty(0).cuda()\n",
    "    y_collect=torch.empty(0).cuda()\n",
    "    \n",
    "    #count=0\n",
    "    for batch in iterator:\n",
    "        \n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc ,_,pred_y= binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss = loss.item()+epoch_loss\n",
    "        epoch_acc = acc.item()+epoch_acc\n",
    "        \n",
    "        pred_collect=torch.cat([pred_collect,pred_y])\n",
    "        y_collect=torch.cat([y_collect,batch.label])\n",
    "        \n",
    "    \n",
    "    #print(pred_collect.shape,y_collect.shape)\n",
    "    \n",
    "    auc=roc_auc_score(y_collect.cpu().data.numpy(),pred_collect.cpu().data.numpy())\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),  auc\n",
    "\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    \n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pred_collect=torch.empty(0).cuda()\n",
    "    y_collect=torch.empty(0).cuda()\n",
    "    y_prob=torch.empty(0).cuda()\n",
    "\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            \n",
    "            acc,pred_y,prob= binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_acc = acc.item()+epoch_acc\n",
    "            pred_collect=torch.cat([pred_collect,pred_y])\n",
    "            y_collect=torch.cat([y_collect,batch.label])\n",
    "            y_prob=torch.cat([y_prob,prob])\n",
    "\n",
    "    try:       \n",
    "        auc=roc_auc_score(y_collect.cpu().data.numpy(),y_prob.cpu().data.numpy())  \n",
    "    except:\n",
    "        auc='UNAVAILABLE'\n",
    "    return epoch_acc / len(iterator),auc,y_collect,y_prob\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_acc=[]\n",
    "val_auc=[]\n",
    "\n",
    "y_lab=None\n",
    "y_pro=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc,train_auc = train(model, train_iterator)\n",
    "    valid_acc ,valid_auc,true_y,prob_y= evaluate(model, valid_iterator)\n",
    "\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print(f'\\t Val. ROC_AUC: {valid_auc*100:.2f}%')\n",
    "\n",
    "    \n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    val_auc.append(valid_auc)\n",
    "\n",
    "    \n",
    "    \n",
    "print(f'time:{time.time()-t:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "_,_,my_lab,my_prob=evaluate(model, valid_iterator)\n",
    "\n",
    "#torch.save(model, 'LSTM_MODEL') save the model\n",
    "\n",
    "pd.DataFrame(val_auc).to_csv('100batch_glove_200_lr_0.005.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_prob=my_prob.cpu().data.numpy()\n",
    "my_lab=my_lab.cpu().data.numpy()\n",
    "np.sort(my_prob)\n",
    "\n",
    "combine=[ [i,j] for i,j in zip(my_prob,my_lab)]\n",
    "\n",
    "combine=sorted(combine)\n",
    "\n",
    "combine=np.array(combine).reshape(len(combine),2)\n",
    "\n",
    "combine=pd.DataFrame(combine,columns=['prob','label'])\n",
    "\n",
    "\n",
    "max_thresh=0\n",
    "max_th_ind=None\n",
    "for th in combine.iloc[:,0]: #threshold\n",
    "    combine['label_with_thresh']=combine['prob'].apply(lambda x: x>th)\n",
    "    combine['label_with_thresh']=combine['label_with_thresh'].astype(int)\n",
    "    combine['final']=(combine['label_with_thresh']==combine['label'])\n",
    "    \n",
    "    if combine['final'].sum()/combine.shape[0] >max_thresh:\n",
    "        max_thresh=combine['final'].sum()/combine.shape[0]\n",
    "        max_th_ind=th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested threshold:    0.3204619884490967\n"
     ]
    }
   ],
   "source": [
    "print('suggested threshold:   ',max_th_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see a tremendous improvement! At least the ROC_AUC curve is no longer 0.5(the minimum).\n",
    "The reason is that, during the training stage, we must let the model learn something from the minor class. If the distribution of data is extremely unbalanced, the classifier is tend to think allocate all samples to the major class. By the GPU and time limitation, we will stop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d=evaluate(model, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521496815286624"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5749780808948891"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() #clear our tinny "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try what if the training dataset is not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTM(my_train,my_test,emb_model=None):\n",
    "\n",
    "    train = my_train\n",
    "    test =my_test\n",
    "\n",
    "    print(train.shape)\n",
    "\n",
    "\n",
    "\n",
    "    train_df, valid_df = train_test_split(train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    import spacy\n",
    "    spacy_en = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "\n",
    "    def tokenizer(text): # create a tokenizer function\n",
    "        # return a list of <class 'spacy.tokens.token.Token'>\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "    TEXT = data.Field(tokenize = tokenizer, include_lengths = True)\n",
    "    LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "\n",
    "\n",
    "    class DataFrameDataset(data.Dataset):\n",
    "\n",
    "        def __init__(self, df, fields, is_test=False, **kwargs):\n",
    "            examples = []\n",
    "            for i, row in df.iterrows():\n",
    "                label = row.Label if not is_test else None\n",
    "                text = row.TEXT\n",
    "                examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "            super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "        @staticmethod\n",
    "        def sort_key(ex):\n",
    "            return len(ex.text)\n",
    "\n",
    "        @classmethod\n",
    "        def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "            train_data, val_data, test_data = (None, None, None)\n",
    "            data_field = fields\n",
    "\n",
    "            if train_df is not None:\n",
    "                print('do train')\n",
    "                train_data = cls(train_df.copy(), data_field, **kwargs)\n",
    "            if val_df is not None:\n",
    "                print('do valid')\n",
    "                val_data = cls(val_df.copy(), data_field, **kwargs)\n",
    "            if test_df is not None:\n",
    "                print('do test')\n",
    "                test_data = cls(test_df.copy(), data_field, **kwargs)\n",
    "\n",
    "            return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n",
    "\n",
    "    fields = [('text',TEXT), ('label',LABEL)]\n",
    "\n",
    "    train_ds, val_ds ,test_ds= DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df,test_df=test)\n",
    "\n",
    "\n",
    "    MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "    TEXT.build_vocab(train_ds, \n",
    "                     max_size = MAX_VOCAB_SIZE, \n",
    "                     vectors = 'glove.6B.50d',\n",
    "                     unk_init = torch.Tensor.zero_)\n",
    "\n",
    "    LABEL.build_vocab(train_ds)\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #device='cpu'\n",
    "\n",
    "    train_iterator, valid_iterator,test_iterator = data.BucketIterator.splits(\n",
    "        (train_ds, val_ds,test_ds), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort_within_batch = True,\n",
    "        device = device)\n",
    "\n",
    "\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "    EMBEDDING_DIM = 50\n",
    "    HIDDEN_DIM = 50\n",
    "    OUTPUT_DIM = 1\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.1\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding\n",
    "\n",
    "    class LSTM_net(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                     bidirectional, dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "\n",
    "            self.rnn = nn.LSTM(embedding_dim, \n",
    "                               hidden_dim, \n",
    "                               num_layers=n_layers, \n",
    "                               bidirectional=bidirectional, \n",
    "                               dropout=dropout)\n",
    "\n",
    "            self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "            self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "            #self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text, text_lengths):\n",
    "\n",
    "            # text = [sent len, batch size]\n",
    "\n",
    "\n",
    "            embedded = self.embedding(text)\n",
    "\n",
    "            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "\n",
    "            packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "            output = self.fc1(hidden)\n",
    "            output = self.fc2(output)\n",
    "\n",
    "            #hidden = [batch size, hid dim * num directions]\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = LSTM_net(INPUT_DIM, \n",
    "                EMBEDDING_DIM, \n",
    "                HIDDEN_DIM, \n",
    "                OUTPUT_DIM, \n",
    "                N_LAYERS, \n",
    "                BIDIRECTIONAL, \n",
    "                DROPOUT, \n",
    "                PAD_IDX)\n",
    "\n",
    "\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    print(pretrained_embeddings.shape)\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    print(model.embedding.weight.data)\n",
    "\n",
    "\n",
    "    model.to(device) #CNN to GPU\n",
    "\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    def binary_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "\n",
    "        #round predictions to the closest integer\n",
    "        #print('pred的shape.......',torch.sigmoid(preds).cpu().data.numpy())\n",
    "        #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        rounded_preds = (torch.sigmoid(preds)>0.32).float()\n",
    "        #print(rounded_preds)\n",
    "        #print(roc_auc_score(y.cpu().data.numpy(),rounded_preds.cpu().data.numpy()))\n",
    "\n",
    "\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "\n",
    "        return acc,rounded_preds,torch.sigmoid(preds)\n",
    "\n",
    "\n",
    "    def train(model, iterator):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        pred_collect=torch.empty(0).cuda()\n",
    "        y_collect=torch.empty(0).cuda()\n",
    "\n",
    "        #count=0\n",
    "        for batch in iterator:\n",
    "\n",
    "            \n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc ,_,pred_y= binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss = loss.item()+epoch_loss\n",
    "            epoch_acc = acc.item()+epoch_acc\n",
    "\n",
    "            pred_collect=torch.cat([pred_collect,pred_y])\n",
    "            y_collect=torch.cat([y_collect,batch.label])\n",
    "\n",
    "\n",
    "        #print(pred_collect.shape,y_collect.shape)\n",
    "\n",
    "        auc=roc_auc_score(y_collect.cpu().data.numpy(),pred_collect.cpu().data.numpy())\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator),  auc\n",
    "\n",
    "\n",
    "    def evaluate(model, iterator):\n",
    "\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        pred_collect=torch.empty(0).cuda()\n",
    "        y_collect=torch.empty(0).cuda()\n",
    "        y_prob=torch.empty(0).cuda()\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                text, text_lengths = batch.text\n",
    "                predictions = model(text, text_lengths).squeeze(1)\n",
    "\n",
    "                #print('预测值',predictions.cpu().data.numpy())\n",
    "\n",
    "                acc,pred_y,prob= binary_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_acc = acc.item()+epoch_acc\n",
    "                pred_collect=torch.cat([pred_collect,pred_y])\n",
    "                y_collect=torch.cat([y_collect,batch.label])\n",
    "                y_prob=torch.cat([y_prob,prob])\n",
    "\n",
    "        try:       \n",
    "            auc=roc_auc_score(y_collect.cpu().data.numpy(),y_prob.cpu().data.numpy())  \n",
    "        except:\n",
    "            auc='UNAVAILABLE'\n",
    "        return epoch_acc / len(iterator),auc,y_collect,y_prob\n",
    "\n",
    "\n",
    "    t = time.time()\n",
    "    loss=[]\n",
    "    acc=[]\n",
    "    val_acc=[]\n",
    "    val_auc=[]\n",
    "\n",
    "    y_lab=None\n",
    "    y_pro=None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_acc,train_auc = train(model, train_iterator)\n",
    "        valid_acc ,valid_auc,true_y,prob_y= evaluate(model, valid_iterator)\n",
    "\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
    "        print(f'\\t Val. ROC_AUC: {valid_auc*100:.2f}%')\n",
    "\n",
    "\n",
    "        loss.append(train_loss)\n",
    "        acc.append(train_acc)\n",
    "        val_acc.append(valid_acc)\n",
    "        val_auc.append(valid_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could modify the build_vocab model as you want, we have tested them, actually, the heavier the pre-trained word vecotors is, the better prediction result is! For example, we tested \"fasttext.en.300d\", it is 6GB large and its performance overwhelmed \"glove_200d\". You can try it by yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "do train\n",
      "do valid\n",
      "do test\n",
      "torch.Size([10002, 50])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
      "        ...,\n",
      "        [-0.1632,  0.7213,  0.2655,  ...,  0.9811, -0.3158, -0.4866],\n",
      "        [ 0.6568,  0.3503,  0.2384,  ...,  0.7875, -0.0827, -0.3559],\n",
      "        [-0.4440, -0.1459,  0.2885,  ..., -0.3518,  0.1557,  1.0669]])\n",
      "\tTrain Loss: 0.287 | Train Acc: 91.51%\n",
      "\t Val. Acc: 91.87%\n",
      "\t Val. ROC_AUC: 51.99%\n",
      "\tTrain Loss: 0.269 | Train Acc: 91.90%\n",
      "\t Val. Acc: 91.74%\n",
      "\t Val. ROC_AUC: 52.83%\n",
      "\tTrain Loss: 0.224 | Train Acc: 91.78%\n",
      "\t Val. Acc: 88.41%\n",
      "\t Val. ROC_AUC: 52.06%\n",
      "\tTrain Loss: 0.169 | Train Acc: 92.98%\n",
      "\t Val. Acc: 86.21%\n",
      "\t Val. ROC_AUC: 53.15%\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.12%\n",
      "\t Val. Acc: 86.27%\n",
      "\t Val. ROC_AUC: 53.90%\n",
      "\tTrain Loss: 0.074 | Train Acc: 97.07%\n",
      "\t Val. Acc: 84.58%\n",
      "\t Val. ROC_AUC: 52.71%\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.18%\n",
      "\t Val. Acc: 86.12%\n",
      "\t Val. ROC_AUC: 53.88%\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.59%\n",
      "\t Val. Acc: 84.89%\n",
      "\t Val. ROC_AUC: 52.94%\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.81%\n",
      "\t Val. Acc: 84.41%\n",
      "\t Val. ROC_AUC: 53.98%\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.02%\n",
      "\t Val. Acc: 84.57%\n",
      "\t Val. ROC_AUC: 53.38%\n"
     ]
    }
   ],
   "source": [
    "unbalance_auc=run_LSTM(df.sample(25000),test) #same test data, same size of the training data, but 1:12 distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This AUC score is lower than balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to try the if the balanced data can perform good even without NLP techniques. We will keep all hyper-parameter as the default set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['len']=train_df['TEXT'].apply(lambda x:len(x))\n",
    "test_df['len']=test_df['TEXT'].apply(lambda x:len(x))\n",
    "\n",
    "\n",
    "cls=LogisticRegression().fit(train_df['len'].values.reshape(-1,1),train_df['Label'])\n",
    "test_n_roc=roc_auc_score(test_df['Label'],cls.predict(test_df['len'].values.reshape(-1,1)))\n",
    "train_n_roc=roc_auc_score(train_df['Label'],cls.predict(train_df['len'].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for simple counting on text len its test and train roc_auc are: ',test_n_roc,train_n_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_df['len'].values.reshape(-1,1),train_df['Label'])\n",
    "test_tree_roc=roc_auc_score(test_df['Label'],cls.predict(test_df['len'].values.reshape(-1,1)))\n",
    "train_tree_roc=roc_auc_score(train_df['Label'],cls.predict(train_df['len'].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for tree model on simple counting on text len its test and train roc_auc are: ',test_tree_roc,train_tree_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the pure counting on length means nothing even we solve the unbalanced data!\n",
    "\n",
    "Next, we convert to simple word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ner_wv']=train_df.TEXT.apply(get_ner_wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ner_wv']=test_df.TEXT.apply(get_ner_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we want to try another pre-trained word2vec which is the spacy's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balan_wv_train=train_df.ner_wv.apply(pd.Series).values\n",
    "balan_wv_test=test_df.ner_wv.apply(pd.Series).values\n",
    "cls=LogisticRegression().fit(balan_wv_train,train_df['Label'])\n",
    "test_n_roc=roc_auc_score(test_df['Label'],cls.predict(balan_wv_test))\n",
    "train_n_roc=roc_auc_score(train_df['Label'],cls.predict(balan_wv_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for tree model on average word vector，its test and train roc_auc are: ',test_n_roc,train_n_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we first performed trivial experiments on unbalanced dataset with different machine learning models. All models performed bad since the unbalanced effect. After then, we rebalanced our training data, as well as involved NLP techniques to process the sequential text data. The step 4 implemented word embedding and LSTM model. We can see if we let the data be balanced, and use a much complex model, the result would be definitely goes better. For example, only simple text length cannot do anything. Then, we applied the average word vector on each sentence. The result demonstrated an improvement. Last, we applied sequential method(LSTM) with word embedding, there was another improvement on the result! The possible explanation are: the word embedding captures some similarity of word, which assigned same features to the same word; the sequential model capture some relationship between some clinical treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
